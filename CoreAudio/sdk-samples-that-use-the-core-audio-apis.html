<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<title>SDK Samples That Use the Core Audio APIs</title>
<style>
table th { border: 1px solid; }
table td { border: 1px solid; }
</style>
</head>
<body>
<hr />
<h2>description: SDK Samples That Use the Core Audio APIs
ms.assetid: 4460df28-a77d-4bf5-9dee-5fb69ba2ded6
title: SDK Samples That Use the Core Audio APIs
ms.topic: article
ms.date: 05/31/2018</h2>
<h1>SDK Samples That Use the Core Audio APIs</h1>
<p>The Windows SDK includes the following code samples that demonstrate the use of the Core Audio APIs. The following samples are located in the directory %MSSdk%\samples\multimedia\audio, where %MSSdk% is the root directory of the Windows SDK installation on your computer.</p>
<table>
<thead>
<tr>
<th>Sample</th>
<th>Deascription</th>
</tr>
</thead>
<tbody>
<tr>
<td><!-- raw HTML omitted -->AECMicArray<!-- raw HTML omitted --></td>
<td>This sample uses the MMDevice, WASAPI, DeviceTopology, and EndpointVolume APIs to capture a high-quality voice stream. The sample supports acoustic echo cancellation (AEC) and microphone array processing by using the AEC DMO also called the Voice capture DSP provided by Microsoft .</td>
</tr>
<tr>
<td><!-- raw HTML omitted -->CaptureSharedEventDriven<!-- raw HTML omitted --></td>
<td>This sample application uses the Core Audio APIs to capture audio data from an input device, specified by the user and writes it to a uniquely named .WAV file in the current directory. This sample demonstrates event-driven buffering.</td>
</tr>
<tr>
<td><!-- raw HTML omitted -->CaptureSharedTimerDriven<!-- raw HTML omitted --></td>
<td>This sample application uses the Core Audio APIs to capture audio data from an input device, specified by the user and writes it to a uniquely named .WAV file in the current directory. This sample demonstrates timer-driven buffering.</td>
</tr>
<tr>
<td><!-- raw HTML omitted -->DuckingCaptureSample<!-- raw HTML omitted --></td>
<td>This sample application demonstrates opening and closing communication streams and causing ducking events that an application can get to implement stream attenuation. This application implements a chat client that uses Core Audio APIs to read audio data from a communication device and to play it on the output device.</td>
</tr>
<tr>
<td><!-- raw HTML omitted -->EndpointVolume<!-- raw HTML omitted --></td>
<td>This sample application uses the Core Audio APIs to change the volume of the device, specified by the user.</td>
</tr>
<tr>
<td><!-- raw HTML omitted -->OSD<!-- raw HTML omitted --></td>
<td>This sample uses the MMDevice and EndpointVolume APIs to implement an on-screen display that shows volume changes to the output stream that plays through the default audio-rendering endpoint device. The on-screen display appears when the user adjusts the volume level in the Windows volume-control program, Sndvol.exe, and it disappears after the volume level remains unchanged for a short period.</td>
</tr>
<tr>
<td><!-- raw HTML omitted -->RenderExclusiveEventDriven<!-- raw HTML omitted --></td>
<td>This sample application uses the Core Audio APIs to render audio data to an output device, specified by the user. This sample demonstrates event-driven buffering for a rendering client in exclusive mode. For an exclusive-mode stream, the client shares the endpoint buffer with the audio device.</td>
</tr>
<tr>
<td><!-- raw HTML omitted -->RenderExclusiveTimerDriven<!-- raw HTML omitted --></td>
<td>This sample application uses the Core Audio APIs to render audio data to an output device, specified by the user. This sample demonstrates timer-driven buffering for a rendering client in exclusive mode. For an exclusive-mode stream, the client shares the endpoint buffer with the audio device.</td>
</tr>
<tr>
<td><!-- raw HTML omitted -->RenderSharedEventDriven<!-- raw HTML omitted --></td>
<td>This sample application uses the Core Audio APIs to render audio data to an output device, specified by the user. This sample demonstrates event-driven buffering for a rendering client in shared mode. For a shared-mode stream, the client shares the endpoint buffer with the audio engine.</td>
</tr>
<tr>
<td><!-- raw HTML omitted -->RenderSharedTimerDriven<!-- raw HTML omitted --></td>
<td>This sample application uses the Core Audio APIs to render audio data to an output device, specified by the user. This sample demonstrates timer-driven buffering for a rendering client in shared mode. For a shared-mode stream, the client shares the endpoint buffer with the audio engine.</td>
</tr>
<tr>
<td>WinAudio</td>
<td>This sample uses the MMDevice API and WASAPI to play and capture audio streams. The user interface of this sample application enables users to select audio endpoint devices, to change the volume level of the local audio session, and to play .wav files and microphone input. <strong>Note:</strong> This sample has been deprecated in WindowsÂ 7.<!-- raw HTML omitted --></td>
</tr>
</tbody>
</table>
<p>Â </p>
<p>You can download the Windows SDK from the <a href="https://developer.microsoft.com/windows/downloads/sdk-archive/">Microsoft Windows SDK Download Center</a> website.</p>
<h2>Related topics</h2>
<!-- raw HTML omitted -->
<p><a href="about-the-windows-core-audio-apis.html">About the Windows Core Audio APIs</a></p>
<!-- raw HTML omitted -->
<p>Â </p>
<p>Â </p>
</body>
